{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP2dwoPjMbNUSbdRg1X6blL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nvwa0318/Deep-learning-model/blob/main/Data_Deep_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoHTVnwB-9V0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "#Data preprocessing\n",
        "\n",
        "#load the dataset\n",
        "dataset = pd.read_csv('insurance.csv') \n",
        "#choose first 7 columns as features\n",
        "features = dataset.iloc[:,0:6] \n",
        "#choose the final column for prediction\n",
        "labels = dataset.iloc[:,-1] \n",
        "\n",
        "#one-hot encoding for categorical variables\n",
        "features = pd.get_dummies(features) \n",
        "#split the data into training and test data\n",
        "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.33, random_state=42) \n",
        " \n",
        "#normalize the numeric columns using ColumnTransformer\n",
        "ct = ColumnTransformer([('normalize', Normalizer(), ['age', 'bmi', 'children'])], remainder='passthrough')\n",
        "#fit the normalizer to the training data and convert from numpy arrays to pandas frame\n",
        "features_train_norm = ct.fit_transform(features_train) \n",
        "#applied the trained normalizer on the test data and convert from numpy arrays to pandas frame\n",
        "features_test_norm = ct.transform(features_test) \n",
        "\n",
        "#ColumnTransformer returns numpy arrays. Convert the features to dataframes\n",
        "features_train_norm = pd.DataFrame(features_train_norm, columns = features_train.columns)\n",
        "features_test_norm = pd.DataFrame(features_test_norm, columns = features_test.columns)\n",
        "\n",
        "# -----> your code here below\n",
        "my_ct=ColumnTransformer([('scale', StandardScaler(), ['age', 'bmi', 'children'])], remainder='passthrough')\n",
        "\n",
        "features_train_scale=my_ct.fit_transform(features_train)\n",
        "features_test_scale=my_ct.transform(features_test)\n",
        "\n",
        "features_train_scale=pd.DataFrame(features_train_scale,columns=features_train.columns)\n",
        "\n",
        "features_test_scale=pd.DataFrame(features_test_scale,columns=features_test.columns)\n",
        "\n",
        "print(features_train_scale.describe())\n",
        "print(features_test_scale.describe())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "def design_model(features):\n",
        "  model=Sequential()\n",
        "  return model\n",
        "\n",
        "  \n",
        "  \n",
        "dataset = pd.read_csv('insurance.csv') #load the dataset\n",
        "features = dataset.iloc[:,0:6] #choose first 7 columns as features\n",
        "labels = dataset.iloc[:,-1] #choose the final column for prediction\n",
        "\n",
        "features = pd.get_dummies(features) #one-hot encoding for categorical variables\n",
        "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.33, random_state=42) #split the data into training and test data\n",
        " \n",
        "#standardize\n",
        "ct = ColumnTransformer([('standardize', StandardScaler(), ['age', 'bmi', 'children'])], remainder='passthrough')\n",
        "features_train = ct.fit_transform(features_train)\n",
        "features_test = ct.transform(features_test)\n",
        "\n",
        "#invoke the function for our model design\n",
        "model = design_model(features_train)\n",
        "\n",
        "#print the layers\n",
        "print(model.layers)"
      ],
      "metadata": {
        "id": "ZPXUo--b_Npe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "layer = layers.Dense(3) #3 is the number we chose\n",
        "\n",
        "print(layer.weights) #we get empty weight and bias arrays because tensorflow doesn't know what the shape is of the input to this layer\n",
        "\n",
        "# 1338 samples, 11 features as in our dataset\n",
        "input = tf.ones((5000, 21)) \n",
        "# a fully-connected layer with 3 neurons\n",
        "layer = layers.Dense(10) \n",
        "# calculate the outputs\n",
        "output = layer(input) \n",
        "# print the weights\n",
        "print(layer.weights) "
      ],
      "metadata": {
        "id": "mw3jLeV2aw9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "def design_model(features):\n",
        "  model = Sequential(name = \"my_first_model\")\n",
        "  #get the number of features/dimensions in the data\n",
        "  num_features = features.shape[1] \n",
        "  #without hard-coding\n",
        "  input = layers.InputLayer(input_shape=(num_features,)) \n",
        "  #adding the input layer\n",
        "\n",
        "  model.add(input) \n",
        "  #your code here\n",
        "  model.add(Dense(128, activation=\"relu\")) #add one hidden layer of 128\n",
        "\n",
        "  #adding an output layer to our model\n",
        "  model.add(Dense(1)) \n",
        "  opt=Adam(learning_rate=0.01)\n",
        "  model.compile(loss='mse',metrics=['mae'],optimizer=opt)\n",
        "  model.fit(features_train,labels_train,epochs=40, batch_size=1,verbose=1)\n",
        "  val_mse,val_mae=model.evaluate(features_test,labels_test,verbose=0)\n",
        "  return model\n",
        "\n",
        "\n",
        "dataset = pd.read_csv('insurance.csv') #load the dataset\n",
        "features = dataset.iloc[:,0:6] #choose first 7 columns as features\n",
        "labels = dataset.iloc[:,-1] #choose the final column for prediction\n",
        "\n",
        "features = pd.get_dummies(features) #one-hot encoding for categorical variables\n",
        "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.33, random_state=42) #split the data into training and test data\n",
        " \n",
        "#standardize\n",
        "ct = ColumnTransformer([('standardize', StandardScaler(), ['age', 'bmi', 'children'])], remainder='passthrough')\n",
        "features_train = ct.fit_transform(features_train)\n",
        "features_test = ct.transform(features_test)\n",
        "\n",
        "#invoke the function for our model design\n",
        "model = design_model(features_train)\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "-EhmEKb-Ssf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tuning of Hyperparameter"
      ],
      "metadata": {
        "id": "ba9b_shdg0yO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = design_model(features_train, learning_rate = 0.01)\n",
        "#your code here\n",
        "model.fit(features_train,labels_train,epochs=40,batch_size=8,verbose=1,validation_split = 0.33)"
      ],
      "metadata": {
        "id": "cooOR0NJgvSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#see model.py file for more details\n",
        "#change the learning rate\n",
        "from model import design_model, features_train, labels_train \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def fit_model(f_train, l_train, learning_rate, num_epochs, bs):\n",
        "    #build the model\n",
        "    model = design_model(f_train, learning_rate)\n",
        "    #train the model on the training data\n",
        "    history = model.fit(f_train, l_train, epochs = num_epochs, batch_size = bs, verbose = 0, validation_split = 0.2)\n",
        "    # plot learning curves\n",
        "    plt.plot(history.history['loss'], label='train')\n",
        "    plt.plot(history.history['val_loss'], label='validation')\n",
        "    plt.title('lrate=' + str(learning_rate))\n",
        "    plt.legend(loc=\"upper right\")\n",
        "\n",
        "\n",
        "#make a list of learning rates to try out\n",
        "learning_rates = [1E-3, 1E-4, 1E-7]\n",
        "#fixed number of epochs\n",
        "num_epochs = 100\n",
        "#fixed number of batches\n",
        "batch_size = 10 \n",
        "\n",
        "for i in range(len(learning_rates)):\n",
        "  plot_no = 420 + (i+1)\n",
        "  plt.subplot(plot_no)\n",
        "  fit_model(features_train, labels_train, learning_rates[i], num_epochs, batch_size)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "plt.savefig('static/images/my_plot.png')\n",
        "print(\"See the plot on the right with learning rates\", learning_rates)\n",
        "import app #don't worry about this. This is to show you the plot in the browser."
      ],
      "metadata": {
        "id": "6tsDwfzzh2yV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#change the batch sizes, with a large batch size, increase the learning rate will help improving performance\n",
        "from model import features_train, labels_train, design_model\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def fit_model(f_train, l_train, learning_rate, num_epochs, batch_size, ax):\n",
        "    model = design_model(features_train, learning_rate)\n",
        "    #train the model on the training data\n",
        "    history = model.fit(features_train, labels_train, epochs=num_epochs, batch_size = batch_size, verbose=0, validation_split = 0.3)\n",
        "    # plot learning curves\n",
        "    ax.plot(history.history['mae'], label='train')\n",
        "    ax.plot(history.history['val_mae'], label='validation')\n",
        "    ax.set_title('batch = ' + str(batch_size), fontdict={'fontsize': 8, 'fontweight': 'medium'})\n",
        "    ax.set_xlabel('# epochs')\n",
        "    ax.set_ylabel('mae')\n",
        "    ax.legend()\n",
        "\n",
        "#fixed learning rate \n",
        "learning_rate = 0.1 \n",
        "#fixed number of epochs\n",
        "num_epochs = 100\n",
        "#we choose a number of batch sizes to try out\n",
        "batches = [4, 32, 64] \n",
        "print(\"Learning rate fixed to:\", learning_rate)\n",
        "\n",
        "#plotting code\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, sharex='col', sharey='row',\n",
        "                        gridspec_kw={'hspace': 0.7, 'wspace': 0.4}) #preparing axes for plotting\n",
        "axes = [ax1, ax2, ax3]\n",
        "\n",
        "#iterate through all the batch values\n",
        "for i in range(len(batches)):\n",
        "  fit_model(features_train, labels_train, learning_rate, num_epochs, batches[i], axes[i])\n",
        "\n",
        "plt.savefig('static/images/my_plot.png')\n",
        "print(\"See the plot on the right with batch sizes\", batches)\n",
        "import app #don't worry about this. This is to show you the plot in the browser."
      ],
      "metadata": {
        "id": "05rLulxSiCZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from model import features_train, labels_train, design_model\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "def fit_model(f_train, l_train, learning_rate, num_epochs):\n",
        "    #build the model: to see the specs go to model.pyl we increased the number of hidden neurons\n",
        "    #in order to introduce some overfitting\n",
        "    model = design_model(features_train, learning_rate) \n",
        "    #train the model on the training data\n",
        "    #your code here\n",
        "    es=EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
        "    history = model.fit(features_train, labels_train, epochs=num_epochs, batch_size= 16, verbose=0, validation_split = 0.2, callbacks = [es])\n",
        "    return history\n",
        "\n",
        "    \n",
        "#using the early stopping in fit_model\n",
        "learning_rate = 0.1\n",
        "num_epochs = 500\n",
        "history = fit_model(features_train, labels_train, learning_rate, num_epochs)\n",
        "\n",
        "#plotting\n",
        "fig, axs = plt.subplots(1, 2, gridspec_kw={'hspace': 1, 'wspace': 0.5}) \n",
        "(ax1, ax2) = axs\n",
        "ax1.plot(history.history['loss'], label='train')\n",
        "ax1.plot(history.history['val_loss'], label='validation')\n",
        "ax1.set_title('lrate=' + str(learning_rate))\n",
        "ax1.legend(loc=\"upper right\")\n",
        "ax1.set_xlabel(\"# of epochs\")\n",
        "ax1.set_ylabel(\"loss (mse)\")\n",
        "\n",
        "ax2.plot(history.history['mae'], label='train')\n",
        "ax2.plot(history.history['val_mae'], label='validation')\n",
        "ax2.set_title('lrate=' + str(learning_rate))\n",
        "ax2.legend(loc=\"upper right\")\n",
        "ax2.set_xlabel(\"# of epochs\")\n",
        "ax2.set_ylabel(\"MAE\")\n",
        "\n",
        "print(\"Final training MAE:\", history.history['mae'][-1])\n",
        "print(\"Final validation MAE:\", history.history['val_mae'][-1])\n",
        "\n",
        "plt.savefig('static/images/my_plot.png')\n",
        "import app #don't worry about this. This is to show you the plot in the browser."
      ],
      "metadata": {
        "id": "8EBhdqw0xZWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# \n",
        "#the validation curve is below the training curve. This means that the training curve can get better at some point, but the model complexity doesn’t allow it. This phenomenon is called underfitting.\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "from model import features_train, labels_train\n",
        "\n",
        "def more_complex_model(X, learning_rate):\n",
        "    model = Sequential(name=\"my_first_model\")\n",
        "    input = tf.keras.Input(shape=(X.shape[1],))\n",
        "    model.add(input)\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "    model.add(layers.Dense(1))\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
        "    model.compile(loss='mse', metrics=['mae'], optimizer=opt)\n",
        "    return model\n",
        "\n",
        "def one_layer_model(X, learning_rate):\n",
        "    model = Sequential(name=\"my_first_model\")\n",
        "    input = tf.keras.Input(shape=(X.shape[1],))\n",
        "    model.add(input)\n",
        "    model.add(layers.Dense(1))\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
        "    model.compile(loss='mse', metrics=['mae'], optimizer=opt)\n",
        "    return model\n",
        "\n",
        "def fit_model(model, f_train, l_train, learning_rate, num_epochs):\n",
        "    #train the model on the training data\n",
        "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = 20)\n",
        "    history = model.fit(features_train, labels_train, epochs=num_epochs, batch_size= 2, verbose=0, validation_split = 0.2, callbacks = [es])\n",
        "    return history\n",
        "\n",
        "def plot(history):\n",
        "    # plot learning curves\n",
        "    fig, axs = plt.subplots(1, 2, gridspec_kw={'hspace': 1, 'wspace': 0.8}) \n",
        "    (ax1, ax2) = axs\n",
        "    ax1.plot(history.history['loss'], label='train')\n",
        "    ax1.plot(history.history['val_loss'], label='validation')\n",
        "    ax1.set_title('lrate=' + str(learning_rate))\n",
        "    ax1.legend(loc=\"upper right\")\n",
        "    ax1.set_xlabel(\"# of epochs\")\n",
        "    ax1.set_ylabel(\"loss (mse)\")\n",
        "\n",
        "    ax2.plot(history.history['mae'], label='train')\n",
        "    ax2.plot(history.history['val_mae'], label='validation')\n",
        "    ax2.set_title('lrate=' + str(learning_rate))\n",
        "    ax2.legend(loc=\"upper right\")\n",
        "    ax2.set_xlabel(\"# of epochs\")\n",
        "    ax2.set_ylabel(\"MAE\")\n",
        "    print(\"Final training MAE:\", history.history['mae'][-1])\n",
        "    print(\"Final validation MAE:\", history.history['val_mae'][-1])\n",
        "\n",
        "learning_rate = 0.1\n",
        "num_epochs = 200\n",
        "\n",
        "#fit the more simple model\n",
        "print(\"Results of a one layer model:\")\n",
        "history1 = fit_model(one_layer_model(features_train, learning_rate), features_train, labels_train, learning_rate, num_epochs)\n",
        "plot(history1)\n",
        "plt.savefig('static/images/my_plot1.png')\n",
        "\n",
        "#fit the more complex model\n",
        "print(\"Results of a model with hidden layers:\")\n",
        "history2 = fit_model(more_complex_model(features_train, learning_rate), features_train, labels_train, learning_rate, num_epochs)\n",
        "plot(history2)\n",
        "plt.savefig('static/images/my_plot2.png')\n",
        "\n",
        "import app #don't worry about this. This is to show you the plot in the browser."
      ],
      "metadata": {
        "id": "kjeHkboZx5HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#gridsearch/randomsearch\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint as sp_randint\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import make_scorer\n",
        "from model import design_model, features_train, labels_train\n",
        "\n",
        "#------------- GRID SEARCH --------------\n",
        "def do_grid_search():\n",
        "  batch_size = [6, 64]\n",
        "  epochs = [10, 50]\n",
        "  model = KerasRegressor(build_fn=design_model)\n",
        "  param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
        "  grid = GridSearchCV(estimator = model, param_grid=param_grid, scoring = make_scorer(mean_squared_error, greater_is_better=False),return_train_score = True)\n",
        "  grid_result = grid.fit(features_train, labels_train, verbose = 0)\n",
        "  print(grid_result)\n",
        "  print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "\n",
        "  means = grid_result.cv_results_['mean_test_score']\n",
        "  stds = grid_result.cv_results_['std_test_score']\n",
        "  params = grid_result.cv_results_['params']\n",
        "  for mean, stdev, param in zip(means, stds, params):\n",
        "      print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
        "\n",
        "  print(\"Traininig\")\n",
        "  means = grid_result.cv_results_['mean_train_score']\n",
        "  stds = grid_result.cv_results_['std_train_score']\n",
        "  for mean, stdev, param in zip(means, stds, params):\n",
        "      print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
        "\n",
        "#------------- RANDOMIZED SEARCH --------------\n",
        "def do_randomized_search():\n",
        "  param_grid = {'batch_size': sp_randint(2, 16), 'nb_epoch': sp_randint(10, 100)}\n",
        "  model = KerasRegressor(build_fn=design_model)\n",
        "  grid = RandomizedSearchCV(estimator = model, param_distributions=param_grid, scoring = make_scorer(mean_squared_error, greater_is_better=False), n_iter = 12)\n",
        "  grid_result = grid.fit(features_train, labels_train, verbose = 0)\n",
        "  print(grid_result)\n",
        "  print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "\n",
        "  means = grid_result.cv_results_['mean_test_score']\n",
        "  stds = grid_result.cv_results_['std_test_score']\n",
        "  params = grid_result.cv_results_['params']\n",
        "  for mean, stdev, param in zip(means, stds, params):\n",
        "      print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
        "\n",
        "print(\"-------------- GRID SEARCH --------------------\")\n",
        "do_grid_search()\n",
        "print(\"-------------- RANDOMIZED SEARCH --------------------\")\n",
        "do_randomized_search()\n",
        "\n"
      ],
      "metadata": {
        "id": "7OwQNZ_j2SaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from model import features_train, labels_train, fit_model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from plotting import plot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def design_model_dropout(X, learning_rate):\n",
        "    model = Sequential(name=\"my_first_model\")\n",
        "    input = tf.keras.Input(shape=(X.shape[1],))\n",
        "    model.add(input)\n",
        "    model.add(layers.Dense(128, activation='relu'))\n",
        "    model.add(layers.Dropout(0.1))\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "    model.add(layers.Dropout(0.2))\n",
        "    model.add(layers.Dense(24, activation='relu'))\n",
        "    #------your code here!------\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Dense(1))\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
        "    model.compile(loss='mse', metrics=['mae'], optimizer=opt)\n",
        "    return model\n",
        "\n",
        "def design_model_no_dropout(X, learning_rate):\n",
        "    model = Sequential(name=\"my_first_model\")\n",
        "    input = layers.InputLayer(input_shape=(X.shape[1],))\n",
        "    model.add(input)\n",
        "    model.add(layers.Dense(128, activation='relu'))\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "    model.add(layers.Dense(24, activation='relu'))\n",
        "    model.add(layers.Dense(1))\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
        "    model.compile(loss='mse', metrics=['mae'], optimizer=opt)\n",
        "    return model\n",
        "    \n",
        "#using the early stopping in fit_model\n",
        "learning_rate = 0.001\n",
        "num_epochs = 200\n",
        "#train the model without dropout\n",
        "history1 = fit_model(design_model_no_dropout(features_train, learning_rate), features_train, labels_train, learning_rate, num_epochs)\n",
        "#train the model with dropout\n",
        "history2 = fit_model(design_model_dropout(features_train, learning_rate), features_train, labels_train, learning_rate, num_epochs)\n",
        "\n",
        "plot(history1, 'static/images/no_dropout.png')\n",
        "\n",
        "plot(history2, 'static/images/with_dropout.png')\n",
        "\n",
        "import app"
      ],
      "metadata": {
        "id": "pVVxKqds4CWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#baseline prediction\n",
        "#see model.py file for more details\n",
        "from model import features_train, labels_train, features_test, labels_test\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.dummy import DummyRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "\n",
        "dummy_regr = DummyRegressor(strategy=\"median\")\n",
        "dummy_regr.fit(features_train, labels_train)\n",
        "y_pred = dummy_regr.predict(features_test)\n",
        "MAE_baseline = mean_absolute_error(labels_test, y_pred)\n",
        "print(MAE_baseline)\n"
      ],
      "metadata": {
        "id": "Y6x6-JtS4u5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "V-P6Jx1o7bhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from tensorflow.keras.models import Sequential# for my_model\n",
        "from tensorflow.keras.layers import InputLayer\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "\n",
        "dataset=pd.read_csv('life_expectancy.csv')\n",
        "dataset.head()\n",
        "dt=dataset.describe()\n",
        "print(dt)\n",
        "\n",
        "dataset=dataset.drop(columns='Country',axis=1)\n",
        "labels=dataset.iloc[:,-1]\n",
        "features=dataset.iloc[:,0:-1]\n",
        "features=pd.get_dummies(features)\n",
        "\n",
        "features_train,features_test,labels_train,labels_test=train_test_split(features,labels,test_size=0.2,random_state=137)\n",
        "\n",
        "numerical_features = features.select_dtypes(include=['float64', 'int64'])\n",
        "numerical_columns = numerical_features.columns\n",
        "ct=ColumnTransformer([('only numeric',StandardScaler(),numerical_columns)],remainder='passthrough')\n",
        "\n",
        "features_train_scaled=ct.fit_transform(features_train)\n",
        "features_test_scaled=ct.transform(features_test)\n",
        "\n",
        "my_model=Sequential()\n",
        "input = layers.InputLayer(input_shape=(features.shape[1],))\n",
        "my_model.add(input)\n",
        "my_model.add(layers.Dense(32,activation='relu'))\n",
        "my_model.add(layers.Dense(1))\n",
        "print(my_model.summary())\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(learning_rate = 0.01)\n",
        "my_model.compile(loss='mse', metrics=['mae'], optimizer=opt)\n",
        "my_model.fit(features_train_scaled,labels_train,epochs=40,batch_size=1,verbose=1)\n",
        "res_mse,res_mae=my_model.evaluate(features_test_scaled,labels_test,verbose=0)\n",
        "print(res_mse)\n",
        "print(res_mae)\n",
        "#print(features)\n",
        "#print(dataset)"
      ],
      "metadata": {
        "id": "yXyUCSQy7E7_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}